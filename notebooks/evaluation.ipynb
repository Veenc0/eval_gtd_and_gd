{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05e9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate bert-score rouge-score mlflow pandas google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62fbf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "import evaluate\n",
    "import bert_score\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a2f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_DIR = Path(\"../data/ground_truth\")\n",
    "GEN_DIR = Path(\"../data/generated_data\")\n",
    "\n",
    "files = [f.stem for f in GT_DIR.glob(\"*.json\") if (GEN_DIR / f\"{f.stem}.json\").exists()]\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "file_ids = []\n",
    "\n",
    "for file_id in files:\n",
    "    with open(GT_DIR / f\"{file_id}.json\") as f:\n",
    "        references.append(json.load(f)[\"abstract\"])\n",
    "\n",
    "    with open(GEN_DIR / f\"{file_id}.json\") as f:\n",
    "        predictions.append(json.load(f)[\"summary\"])\n",
    "\n",
    "    file_ids.append(file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41807afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE: {'rouge1': np.float64(0.24064511890684534), 'rouge2': np.float64(0.051802790301772865), 'rougeL': np.float64(0.14304296413086323), 'rougeLsum': np.float64(0.14304296413086323)}\n",
      "BERTScore: [0.8158199787139893, 0.8339284062385559]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bert_results = bert.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "print(\"ROUGE:\", rouge_results)\n",
    "print(\"BERTScore:\", bert_results[\"f1\"][:5])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc8abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def gemini_llm_score(reference, prediction):\n",
    "    prompt = f\"\"\"\n",
    "You are an evaluation assistant. Compare the generated summary to the reference abstract.\n",
    "\n",
    "Return a JSON object with:\n",
    "- similarity_score: an integer from 1 to 10. 1 being the lowest similarity and 10 being the highest between the generated summary and the reference abstract.\n",
    "- correct: true or false\n",
    "- reason: a short explanation on why you decided on the similarity score and how you determined if the summary is correct or not.\n",
    "\n",
    "Important: respond in valid JSON only. Do not include markdown or explanation outside the JSON.\n",
    "\n",
    "Abstract:\n",
    "{reference}\n",
    "\n",
    "Generated Summary:\n",
    "{prediction}\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\")\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        reply = response.text.strip()\n",
    "        json_start = reply.find('{')\n",
    "        json_end = reply.rfind('}') + 1\n",
    "        json_str = reply[json_start:json_end]\n",
    "\n",
    "        parsed = json.loads(json_str)\n",
    "        return parsed.get(\"similarity_score\"), parsed.get(\"correct\"), parsed.get(\"reason\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"LLM eval error: {e}\")\n",
    "        return None, None, f\"LLM error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a86934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bert_f1</th>\n",
       "      <th>llm_similarity</th>\n",
       "      <th>llm_correct</th>\n",
       "      <th>llm_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does_the_categorization_difficulty_elicit_the_uncanny-valley-like_phenomenon_without_animacy</td>\n",
       "      <td>0.240645</td>\n",
       "      <td>0.143043</td>\n",
       "      <td>0.815820</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated summary accurately captures the main points of the abstract.  It correctly identifies the study's aim to investigate the uncanny valley effect using non-animate objects, the methodology involving morphing shapes and measuring categorization time and likeability, and the key finding that categorization difficulty, not processing fluency, is the primary driver of the effect. The summary concisely summarizes the experiments and their outcomes, aligning well with the abstract's core message.  The slight difference in the level of detail doesn't detract significantly from the overall accuracy and similarity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VerilogReader_LLM-Aided_Hardware_Test_Generation</td>\n",
       "      <td>0.240645</td>\n",
       "      <td>0.143043</td>\n",
       "      <td>0.833928</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated summary accurately captures the main points of the abstract: using LLMs for coverage-directed test generation, the improvement over random testing, and limitations with complex designs.  However, it introduces additional details (e.g., 'Coverage Explainer', 'DUT Explainer', JSON format) not explicitly mentioned in the abstract, and it omits the mention of prompt engineering optimization. The score reflects the overall accuracy and completeness but considers the lack of perfect alignment with the source abstract.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           file  \\\n",
       "0  Does_the_categorization_difficulty_elicit_the_uncanny-valley-like_phenomenon_without_animacy   \n",
       "1                                              VerilogReader_LLM-Aided_Hardware_Test_Generation   \n",
       "\n",
       "     rouge1    rougeL   bert_f1  llm_similarity  llm_correct  \\\n",
       "0  0.240645  0.143043  0.815820               9         True   \n",
       "1  0.240645  0.143043  0.833928               7         True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         llm_reason  \n",
       "0  The generated summary accurately captures the main points of the abstract.  It correctly identifies the study's aim to investigate the uncanny valley effect using non-animate objects, the methodology involving morphing shapes and measuring categorization time and likeability, and the key finding that categorization difficulty, not processing fluency, is the primary driver of the effect. The summary concisely summarizes the experiments and their outcomes, aligning well with the abstract's core message.  The slight difference in the level of detail doesn't detract significantly from the overall accuracy and similarity.  \n",
       "1                                                                                               The generated summary accurately captures the main points of the abstract: using LLMs for coverage-directed test generation, the improvement over random testing, and limitations with complex designs.  However, it introduces additional details (e.g., 'Coverage Explainer', 'DUT Explainer', JSON format) not explicitly mentioned in the abstract, and it omits the mention of prompt engineering optimization. The score reflects the overall accuracy and completeness but considers the lack of perfect alignment with the source abstract.  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    pred = predictions[i]\n",
    "    ref = references[i]\n",
    "    file_id = file_ids[i]\n",
    "\n",
    "    sim_score, correct, reason = gemini_llm_score(ref, pred)\n",
    "\n",
    "    result = {\n",
    "        \"file\": file_id,\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"bert_f1\": bert_results[\"f1\"][i],\n",
    "        \"llm_similarity\": sim_score,\n",
    "        \"llm_correct\": correct,\n",
    "        \"llm_reason\": reason\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()\n",
    "\n",
    "# Daten im csv speichern - dont know if this is needed\n",
    "# output_csv_path = Path(\"../data/evaluation_results.csv\") \n",
    "# df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "990d47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with mlflow.start_run():\n",
    "    for row in results:\n",
    "        file = row[\"file\"]\n",
    "        mlflow.log_metric(f\"{file}_rouge1\", row[\"rouge1\"])\n",
    "        mlflow.log_metric(f\"{file}_rougeL\", row[\"rougeL\"])\n",
    "        mlflow.log_metric(f\"{file}_bert_f1\", row[\"bert_f1\"])\n",
    "\n",
    "        if row[\"llm_similarity\"] is not None:\n",
    "            mlflow.log_metric(f\"{file}_llm_similarity\", row[\"llm_similarity\"])\n",
    "\n",
    "        if row[\"llm_correct\"] is not None:\n",
    "            mlflow.set_tag(f\"{file}_llm_correct\", row[\"llm_correct\"])\n",
    "\n",
    "        mlflow.set_tag(f\"{file}_llm_reason\", row[\"llm_reason\"])\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf81f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt zum directory /notebooks wechseln\n",
    "# und dann  \"mlflow ui\" im Terminal ausführen (it only runs locally)\n",
    "# und im Browser auf http://localhost:5000 gehen, um die Metriken zu sehen. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
